services:
  agent:
    build: .
    container_name: content_agent
    restart: unless-stopped  
    depends_on:
      vllm:
        condition: service_healthy
    volumes:
      - ./data:/app/data
    env_file:
      - .env
    environment:
      - OPENAI_API_BASE=http://vllm:8000/v1
      - LOG_LEVEL=INFO
    command: ["python", "-m", "src.content_agents.main", "--loop", "--interval", "3600"]

  vllm:
    image: vllm/vllm-openai:latest
    container_name: vllm_server
    runtime: nvidia
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    ports:
      - "8000:8000" 
    ipc: host
    command: >
      --model ${MODEL_NAME}
      --api-key ${VLLM_API_KEY}
      --gpu-memory-utilization ${GPU_MEMORY_UTILIZATION}
      --max-model-len ${MAX_MODEL_LEN}
      --dtype ${DTYPE}
      --trust-remote-code
      --enforce-eager
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 20s
      timeout: 10s
      retries: 20
