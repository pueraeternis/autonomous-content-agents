services:
  # App
  app:
    build: .
    volumes:
      - ./src:/app/src
      - ./config:/app/config
      - ./data:/app/data
    environment:
      - OPENAI_API_KEY=${VLLM_API_KEY}
      - OPENAI_API_BASE=http://vllm:8000/v1
    depends_on:
      vllm:
        condition: service_healthy
    command: tail -f /dev/null

  # vLLM Server
  vllm:
    image: vllm/vllm-openai:latest
    runtime: nvidia
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    ports:
      - "8000:8000"
    ipc: host
    command: >
      --model ${MODEL_NAME}
      --api-key ${VLLM_API_KEY}
      --gpu-memory-utilization ${GPU_MEMORY_UTILIZATION}
      --max-model-len ${MAX_MODEL_LEN}
      --dtype ${DTYPE}
      --trust-remote-code
      --enforce-eager
    
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 20s
      timeout: 10s
      retries: 20